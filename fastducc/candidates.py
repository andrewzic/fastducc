# -*- coding: utf-8 -*-
import argparse
import glob
import os
import shutil
import itertools
import sys
import re
from typing import Iterable, Tuple, List, Dict, Any, Optional
from dataclasses import dataclass

from numba import njit, prange
import numpy as np
import math
from tqdm import tqdm

import astropy.constants as const
import astropy.units as u
from astropy.coordinates import SkyCoord
from astropy.visualization.wcsaxes import WCSAxes
from astropy.wcs import WCS
from astropy.io import fits
from astropy.table import Table, vstack

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.animation import FuncAnimation, PillowWriter
from ligo.skymap.plot.marker import reticle

from scipy.ndimage import maximum_filter

from casacore.tables import table
try:
    import ducc0
except Exception as e:
    raise RuntimeError('ducc0 is required') from e

from fastducc import wcs as ducc_wcs
from fastducc import filters, kernels, ms_utils, detection

def make_stdmap_snippet(
    std_map: np.ndarray,                # (Ny, Nx)
    candidate: Dict[str, Any],
    spatial_size: int = 50
) -> Dict[str, Any]:
    """
    Create a snippet record from a 2-D variance/std map.
    Time length is 1 (single frame), with NaN time stamp for compatibility.
    """
    Ny, Nx = std_map.shape
    y = int(candidate["y"]); x = int(candidate["x"])
    half_sp = spatial_size // 2

    y0 = max(0, y - half_sp)
    y1 = min(Ny, y0 + spatial_size)
    x0 = max(0, x - half_sp)
    x1 = min(Nx, x0 + spatial_size)

    cut = std_map[y0:y1, x0:x1]
    # shape should be exactly (spatial_size, spatial_size); pad if cropped at edges
    pad_y_top = max(0, (y - half_sp) - y0)
    pad_y_bot = max(0, (y0 + spatial_size) - y1)
    pad_x_left = max(0, (x - half_sp) - x0)
    pad_x_right = max(0, (x0 + spatial_size) - x1)

    snippet_2d = np.pad(cut,
                        ((pad_y_top, pad_y_bot), (pad_x_left, pad_x_right)),
                        mode="edge")
    # Time length = 1
    snippet_cube = snippet_2d[None, :, :]  # (1, spatial_size, spatial_size)
    snippet_times = np.array([np.nan], dtype=float)

    rec = {
        "candidate": candidate,
        "snippet_cube": snippet_cube.astype(np.float32),
        "snippet_times": snippet_times,
        "meta": {
            "time_indices": {"desired": (0, 1), "clipped": (0, 1), "pad": (0, 0)},
            "y_indices":    {"desired": (y - half_sp, y - half_sp + spatial_size),
                             "clipped": (y0, y1), "pad": (pad_y_top, pad_y_bot)},
            "x_indices":    {"desired": (x - half_sp, x - half_sp + spatial_size),
                             "clipped": (x0, x1), "pad": (pad_x_left, pad_x_right)},
            "center_idx": 0,
            "snippet_shape": (1, spatial_size, spatial_size),
        }
    }
    return rec


def extract_candidate_snippets(
    times: np.ndarray,
    cube: np.ndarray,
    detections: List[Dict[str, Any]],
    *,
    spatial_size: int = 50,
    time_factor: int = 25,            # in units of *smoothed* frames
    pad_mode: str = "constant",
    pad_value: float = 0.0,
    return_indices: bool = True,
    center_policy: str = "right",    # 'right' uses w//2, 'left' uses (w-1)//2 for center time
) -> List[Dict[str, Any]]:
    """
    Extract candidate snippets from a *boxcar-smoothed* version of the cube,
    using the candidate width as the temporal averaging window.

    Parameters
    ----------
    times : (T,) ndarray
        Original time stamps (seconds or any monotonically increasing floats).
    cube : (T, Ny, Nx) ndarray
        Original full-resolution image cube.
    detections : list of dict
        Filtered candidates (already annotated with sky coordinates), each with at least:
           'y','x' : int pixel coordinates in the full image
           'width_samples' : int boxcar width (samples)
           't0_idx' : int window start index (recommended)
           or 'center_idx' : int original center sample (falls back if t0_idx missing)
    spatial_size : int
        Spatial crop size (square cutout).
    time_factor : int
        Snippet temporal length measured in *smoothed* frames (not original frames).
        Final snippet length is `t_len_sm = max(1, time_factor)`.
    pad_mode : {"constant","edge"}
        Padding mode to enforce fixed snippet shape.
    pad_value : float
        Constant padding value if pad_mode="constant".
    return_indices : bool
        If True, include index bookkeeping in the "meta" dictionary.
    center_policy : {"right","left"}
        Policy for defining the *center time* of each w-sample window:
          - "right": center = t0 + w//2 (default; consistent with previous fixes)
          - "left":  center = t0 + (w-1)//2

    Returns
    -------
    out : list of dict
        Each record:
          {
            "candidate": det_with_extra_fields,
            "snippet_cube": (t_len_sm, spatial_size, spatial_size) array,
            "snippet_times": (t_len_sm,) array,
            "meta": {...}  # if return_indices=True
          }
    """
    # --- validate inputs ---
    if cube.ndim != 3:
        raise ValueError("cube must be (T, Ny, Nx)")
    T, Ny, Nx = cube.shape
    if times.shape[0] != T:
        raise ValueError("times length must match cube time axis")
    if spatial_size < 1 or time_factor < 1:
        raise ValueError("spatial_size and time_factor must be >= 1")

    # --- spatial center index within snippet ---
    half_sp = spatial_size // 2

    # --- Precompute cumulative sum for fast boxcar smoothing (once) ---
    # Use float64 for numerical stability (same dtype as numba kernels).
    csum = np.zeros((T + 1, Ny, Nx), dtype=np.float64)
    csum[1:] = np.cumsum(cube.astype(np.float64, copy=False), axis=0)


    # --- Build smoothed cubes & times per unique width ---
    unique_w = sorted(set(max(1, int(d.get("width_samples", 1))) for d in detections))
    smoothed_by_w: Dict[int, Dict[str, Any]] = {}

    for w in unique_w:
        if w > T:
            # pathological: window longer than data
            sm_cube = np.empty((0, Ny, Nx), dtype=np.float64)
            sm_times = np.empty((0,), dtype=times.dtype)
        else:
            S_sum = kernels._moving_sum_from_csum(csum, w)  # (T_eff, Ny, Nx) windowed sums
            sm_cube = S_sum / float(w)
            T_eff = sm_cube.shape[0]
            # Choose center time index inside each window
            offset = (w // 2) if (center_policy == "right") else ((w - 1) // 2)
            # `times_sm[t0] = times[t0 + offset]`
            start = offset
            end = start + T_eff
            sm_times = times[start:end]
        smoothed_by_w[w] = {"cube": sm_cube, "times": sm_times}

    out: List[Dict[str, Any]] = []

    # --- helper to resolve indices per detection ---
    def _resolve_w(det: Dict[str, Any]) -> int:
        return max(1, int(det.get("width_samples", 1)))

    def _resolve_sm_center_idx(det: Dict[str, Any], w: int) -> int:
        """
        Center index in *smoothed* series. Prefer t0_idx if present; otherwise
        map from original center_idx back to window start.
        """
        if "t0_idx" in det:
            return int(det["t0_idx"])
        # Fallback: derive t0_idx from center_idx and width
        c = int(det.get("center_idx", 0))
        # For 'right' center, center_idx = t0 + w//2  -> t0 = center_idx - w//2
        # For 'left'  center, center_idx = t0 + (w-1)//2
        if center_policy == "right":
            return max(0, c - (w // 2))
        else:
            return max(0, c - ((w - 1) // 2))

    # --- main loop over detections ---
    for det in detections:
        y = int(det["y"]); x = int(det["x"])
        w = _resolve_w(det)

        sm = smoothed_by_w[w]
        sm_cube = sm["cube"]         # shape: (T_eff, Ny, Nx)
        sm_times = sm["times"]       # shape: (T_eff,)
        T_eff = sm_cube.shape[0]

        # If width exceeds data length, return an all-NaN snippet
        if T_eff == 0:
            t_len_sm = max(1, time_factor)
            snippet_cube = np.full((t_len_sm, spatial_size, spatial_size), np.nan, dtype=np.float32)
            snippet_times = np.full((t_len_sm,), np.nan, dtype=times.dtype)
            rec = {"candidate": det, "snippet_cube": snippet_cube, "snippet_times": snippet_times}
            if return_indices:
                rec["meta"] = {
                    "time_indices": {"desired": (0, t_len_sm), "clipped": (0, 0), "pad": (t_len_sm, 0)},
                    "y_indices": {"desired": (y - half_sp, y - half_sp + spatial_size),
                                  "clipped": (0, 0), "pad": (spatial_size, 0)},
                    "x_indices": {"desired": (x - half_sp, x - half_sp + spatial_size),
                                  "clipped": (0, 0), "pad": (spatial_size, 0)},
                    "center_idx_smoothed": 0,
                    "snippet_shape": (t_len_sm, spatial_size, spatial_size),
                }
            out.append(rec)
            continue

        # Smoothed center index (frame in smoothed series)
        k_center = _resolve_sm_center_idx(det, w)

        # Snippet length in *smoothed frames*
        t_len_sm = max(1, time_factor)
        half_t = t_len_sm // 2

        # Desired smoothed time window around k_center
        k0 = k_center - half_t
        k1 = k0 + t_len_sm  # exclusive

        # Clip to valid smoothed indices
        k0_clip = max(0, k0)
        k1_clip = min(T_eff, k1)

        # Spatial desired indices (same as before, about full-resolution pixel location)
        y0 = y - half_sp
        y1 = y0 + spatial_size
        x0 = x - half_sp
        x1 = x0 + spatial_size

        # Clip in space
        y0_clip = max(0, y0)
        y1_clip = min(Ny, y1)
        x0_clip = max(0, x0)
        x1_clip = min(Nx, x1)

        # Required padding to keep fixed shapes
        pad_t_front = k0_clip - k0
        pad_t_back  = k1 - k1_clip
        pad_y_top   = y0_clip - y0
        pad_y_bot   = y1 - y1_clip
        pad_x_left  = x0_clip - x0
        pad_x_right = x1 - x1_clip

        # Slice smoothed region
        sub_cube  = sm_cube[k0_clip:k1_clip, y0_clip:y1_clip, x0_clip:x1_clip]
        sub_times = sm_times[k0_clip:k1_clip]

        # Pad to fixed shapes
        pad_widths = (
            (pad_t_front, pad_t_back),
            (pad_y_top,   pad_y_bot),
            (pad_x_left,  pad_x_right),
        )

        if pad_mode == "constant":
            snippet_cube = np.pad(sub_cube, pad_widths, mode="constant", constant_values=pad_value)
            snippet_times = np.pad(sub_times, (pad_t_front, pad_t_back),
                                   mode="constant", constant_values=np.nan)
        elif pad_mode == "edge":
            snippet_cube = np.pad(sub_cube, pad_widths, mode="edge")
            if sub_times.size == 0:
                snippet_times = np.full((t_len_sm,), np.nan, dtype=times.dtype)
            else:
                front_vals = np.full((pad_t_front,), sub_times[0], dtype=sub_times.dtype)
                back_vals  = np.full((pad_t_back,),  sub_times[-1], dtype=sub_times.dtype)
                snippet_times = np.concatenate([front_vals, sub_times, back_vals], axis=0)
        else:
            raise ValueError(f"Unknown pad_mode='{pad_mode}'")

        # Enforce exact shapes
        if snippet_cube.shape != (t_len_sm, spatial_size, spatial_size):
            raise RuntimeError(f"Snippet cube has unexpected shape {snippet_cube.shape}")
        if snippet_times.shape != (t_len_sm,):
            raise RuntimeError(f"Snippet times has unexpected shape {snippet_times.shape}")

        # Optional: local peak search around the smoothed center frame
        lc = snippet_cube[:, half_sp, half_sp]  # detection pixel (center)
        good = np.isfinite(snippet_times) & np.isfinite(lc)
        if np.any(good):
            rel_peak = np.nanargmax(lc[good])
            # Map back to snippet indices: among good only; use direct max over snippet for simplicity
            rel_peak_all = int(np.nanargmax(lc))
            det_time_peak = snippet_times[rel_peak_all]
            det["time_center_peak_smoothed"] = float(det_time_peak) if np.isfinite(det_time_peak) else float(det.get("time_center", np.nan))
            det["center_idx_peak_smoothed"]  = int(k_center + (rel_peak_all - half_t))

        # Record smoothed center time (for plotting)
        det["time_center_smoothed"] = float(sm_times[k_center]) if (0 <= k_center < sm_times.shape[0]) else float(det.get("time_center", np.nan))
        det["center_idx_smoothed"]  = int(k_center)

        # Prepare output record
        rec: Dict[str, Any] = {
            "candidate": det,
            "snippet_cube": snippet_cube.astype(np.float32, copy=False),
            "snippet_times": snippet_times,
        }

        if return_indices:
            rec["meta"] = {
                "time_indices": {"desired": (k0, k1), "clipped": (k0_clip, k1_clip), "pad": (pad_t_front, pad_t_back)},
                "y_indices":    {"desired": (y0, y1), "clipped": (y0_clip, y1_clip), "pad": (pad_y_top, pad_y_bot)},
                "x_indices":    {"desired": (x0, x1), "clipped": (x0_clip, x1_clip), "pad": (pad_x_left, pad_x_right)},
                "center_idx_smoothed": int(k_center),
                "snippet_shape": (t_len_sm, spatial_size, spatial_size),
                "width_samples": int(w),
            }

        out.append(rec)

    return out

def candidates_to_astropy_table(annotated: List[Dict[str, Any]]) -> Table:
    """
    Convert the list of annotated candidate dicts into an Astropy Table with units.

    Expected keys per candidate dict (from annotate_candidates_with_sky_coords):
      'x','y','l','m','ra_rad','dec_rad','ra_hms','dec_dms','snr','width_samples',
      'time_start','time_end','time_center','t0_idx','t1_idx_excl','center_idx',
      'phase_center_field' (some keys may be missing depending on pipeline)

    Returns
    -------
    t : astropy.table.Table
        Table with appropriate columns and units.
    """
    if len(annotated) == 0:
        # return an empty table with typical schema
        t = Table(names=[
            "srcname","x","y","l","m","ra_rad","dec_rad","ra_deg","dec_deg",
            "ra_hms","dec_dms","snr","width_samples",
            "time_start","time_end","time_center",
            "t0_idx","t1_idx_excl","center_idx","phase_center_field"
        ], dtype=[
            str, int,int,float,float,float,float,float,float,
            "U20","U20",float,int,
            float,float,float,
            int,int,int,"U64"
        ])
        return t

    # Build arrays column-wise, handling missing keys gracefully
    def col(key, default=np.nan):
        return [c.get(key, default) for c in annotated]

    # Base numeric columns
    srcname = np.array(col("srcname", default=""), dtype=str)
    xs  = np.array(col("x", default=np.int64(-1)), dtype=np.int64)
    ys  = np.array(col("y", default=np.int64(-1)), dtype=np.int64)
    ls  = np.array(col("l"), dtype=float)
    ms  = np.array(col("m"), dtype=float)
    ra_rad = np.array(col("ra_rad"), dtype=float)
    dec_rad = np.array(col("dec_rad"), dtype=float)

    # Derived in degrees for convenience
    ra_deg  = np.degrees(ra_rad)
    dec_deg = np.degrees(dec_rad)

    # Strings for sexagesimal
    ra_hms  = np.array(col("ra_hms", default=""), dtype=str)
    dec_dms = np.array(col("dec_dms", default=""), dtype=str)

    # Optional metrics
    snr          = np.array(col("snr"), dtype=float)
    width_samples= np.array(col("width_samples", default=np.int64(-1)), dtype=np.int64)

    # Times & indices (float seconds, int indices)
    time_start   = np.array(col("time_start"), dtype=float)
    time_end     = np.array(col("time_end"), dtype=float)
    time_center  = np.array(col("time_center"), dtype=float)

    t0_idx       = np.array(col("t0_idx", default=np.int64(-1)), dtype=np.int64)
    t1_idx_excl  = np.array(col("t1_idx_excl", default=np.int64(-1)), dtype=np.int64)
    center_idx   = np.array(col("center_idx", default=np.int64(-1)), dtype=np.int64)

    # Field name (string)
    field_name   = np.array(col("phase_center_field", default=""), dtype=str)

    # Build Astropy Table
    t = Table()
    t["srcname"] = srcname
    t["x"] = xs
    t["y"] = ys
    t["l"] = ls * u.dimensionless_unscaled          # direction cosine
    t["m"] = ms * u.dimensionless_unscaled          # direction cosine
    t["ra_rad"]  = ra_rad * u.rad
    t["dec_rad"] = dec_rad * u.rad
    t["ra_deg"]  = ra_deg * u.deg
    t["dec_deg"] = dec_deg * u.deg
    t["ra_hms"]  = ra_hms
    t["dec_dms"] = dec_dms
    t["snr"]     = snr
    t["width_samples"] = width_samples
    t["time_start"]  = time_start * u.s
    t["time_end"]    = time_end * u.s
    t["time_center"] = time_center * u.s
    t["t0_idx"]      = t0_idx
    t["t1_idx_excl"] = t1_idx_excl
    t["center_idx"]  = center_idx
    t["phase_center_field"] = field_name

    return t

def save_candidates_table(table: Table, csv_path: str, vot_path: str) -> None:
    """
    Save the Astropy Table to CSV and VOTable.
    - CSV: plain text; units appear in metadata but not in numeric values.
    - VOTable: includes unit information in FIELDs where available.

    Parameters
    ----------
    table : astropy.table.Table
        The table to write.
    csv_path : str
        Destination CSV file path.
    vot_path : str
        Destination VOTable file path (usually .vot or .xml).
    """
    # CSV
    table.write(csv_path, format="csv", overwrite=True)

    # VOTable (Astropy handles units and metadata)
    table.write(vot_path, format="votable", overwrite=True)


def save_candidate_lightcurves(
    times: np.ndarray,
    cube: np.ndarray,
    candidate: Dict[str, Any],
    out_prefix: str,
    std_map: Optional[np.ndarray] = None,
    use_std_images: bool = False,
    *,
    spatial_size: int = 50,
    save_format: str = "npz",        # "npz" or "ascii"
    center_policy: str = "right",    # "right" uses w//2; "left" uses (w-1)//2
    cmap: str = "gray",
    dpi: int = 200,
    # WCS parameters:
    npix_x: int = None, npix_y: int = None,  # full image size (defaults from cube if None)
    ra0_rad: float = None, dec0_rad: float = None,  # phase center (radians)
    pix_rad: float = None,                  # pixel scale (radians/pixel) from CLI
    ra_sign: int = -1,                      # -1 => RA increases left; +1 => RA increases right
    dec_sign: int = 1,
    radesys: str = "ICRS", equinox: float = None
) -> Dict[str, str]:


    """
    Extract and save full-resolution and boxcar-smoothed light curves for a candidate,
    and produce a WCS-labelled figure:
      - Top-left: full frame at detection time (WCS RA/Dec)
      - Top-right: cutout/snippet around candidate (WCS RA/Dec)
      - Middle: full-res LC
      - Bottom: boxcar-smoothed LC

    Called before snippet products are produced.

    Returns dict of file paths.
    """

    
    # --- Validate inputs ---
    if cube.ndim != 3:
        raise ValueError("cube must be (T, Ny, Nx)")
    T, Ny, Nx = cube.shape
    if times.shape[0] != T:
        raise ValueError("times length must match cube time axis")
    if spatial_size < 1:
        raise ValueError("spatial_size must be >= 1")
    if save_format not in ("npz", "ascii"):
        raise ValueError("save_format must be 'npz' or 'ascii'")
    if pix_rad is None or ra0_rad is None or dec0_rad is None:
        raise ValueError("You must pass ra0_rad/dec0_rad (phase center) and pix_rad (CLI pixel scale).")

    # Default full image dims from cube if not provided
    if npix_x is None: npix_x = Nx
    if npix_y is None: npix_y = Ny

    # --- Candidate fields ---
    y = int(candidate["y"])
    x = int(candidate["x"])
    w = max(1, int(candidate.get("width_samples", 1)))

    # Preferred smoothed start index, else derive from center_idx
    if "t0_idx" in candidate:
        
        t0_idx = int(candidate["t0_idx"])
        
    else:
        cidx = int(candidate.get("center_idx", 0))
        t0_idx = cidx - (w // 2) if (center_policy == "right") else cidx - ((w - 1) // 2)
        t0_idx = max(0, min(t0_idx, T - w))

    offset = (w // 2) if (center_policy == "right") else ((w - 1) // 2)
    t_full_center = max(0, min(t0_idx + offset, T - 1))

    # --- Full-res light curve at the candidate pixel ---
    lc_full = cube[:, y, x].astype(np.float64, copy=False)

    # --- Boxcar-smoothed light curve ---
    lc_sm, T_eff = kernels._compute_boxcar_1d(lc_full, w)
    if T_eff > 0:
        times_sm = times[offset:offset + T_eff]
        k_center = max(0, min(t0_idx, T_eff - 1))  # smoothed index of detection window start
    else:
        times_sm = np.empty((0,), dtype=times.dtype)
        k_center = 0

    # --- Images to display ---

    if use_std_images and (std_map is not None):
        # Use std_map for both full frame and cutout
        frame_for_display = std_map
    else:
        frame_for_display = cube[t_full_center]
        
    vmin = np.nanpercentile(frame_for_display, 5.0)
    vmax = np.nanpercentile(frame_for_display, 99.5)
    
    # Full-frame WCS:
    wcs_full = ducc_wcs._build_fullframe_wcs(npix_x=npix_x, npix_y=npix_y,
                                    ra0_rad=ra0_rad, dec0_rad=dec0_rad,
                                    pixscale_rad=pix_rad,
                                    ra_sign=ra_sign, radesys=radesys, equinox=equinox)


    if "ra_rad" in candidate and "dec_rad" in candidate:
        ra_c = float(candidate["ra_rad"])
        dec_c = float(candidate["dec_rad"])
    else:

        ra_c = ra0_rad
        dec_c = dec0_rad
    
    # Snippet/cutout for the chosen frame
    half_sp = spatial_size // 2
    y0 = max(0, y - half_sp); y1 = min(Ny, y0 + spatial_size)
    x0 = max(0, x - half_sp); x1 = min(Nx, x0 + spatial_size)
    snippet = frame_for_display[y0:y1, x0:x1]
    
    # Snippet WCS centered on candidate world coords
    wcs_cut = ducc_wcs._build_tan_wcs_for_snippet(spatial_size=snippet.shape[0],
                                         ra_rad=ra_c, dec_rad=dec_c,
                                         pixscale_rad=pix_rad,
                                         ra_sign=ra_sign, radesys=radesys, equinox=equinox)



    # --- Save light curves ---
    out_full = f"{out_prefix}_lc_full.{ 'npz' if save_format=='npz' else 'txt' }"
    out_box  = f"{out_prefix}_lc_boxcar_w{w}.{ 'npz' if save_format=='npz' else 'txt' }"

    if save_format == "npz":
        np.savez(out_full, time=times, flux=lc_full)
        np.savez(out_box, time=times_sm, flux=lc_sm, width=w)
    else:
        # ASCII: two columns "time flux" with header
        with open(out_full, "w") as f:
            f.write("# time  flux(full_res)\n")
            for t, v in zip(times, lc_full):
                f.write(f"{t:.9f} {v:.9e}\n")
        with open(out_box, "w") as f:
            f.write(f"# time  flux(boxcar_mean_w={w})\n")
            for t, v in zip(times_sm, lc_sm):
                f.write(f"{t:.9f} {v:.9e}\n")

    
    
    # --- Build the figure ---
    out_fig = f"{out_prefix}_lightcurves.png"
    fig = plt.figure(figsize=(10, 10), dpi=dpi)

    gs = GridSpec(
        nrows=3, ncols=2, figure=fig,
        height_ratios=[1.1, 1.0, 1.0],   # give images slightly more height
        width_ratios=[1, 1],
    )

    
    
    # Top-left: WCS full frame
    ax_full = fig.add_subplot(gs[0,0], projection=wcs_full)
    im_full = ax_full.imshow(frame_for_display, origin="lower", cmap=cmap, vmin=vmin, vmax=vmax)
    ax_full.plot([x], [y], marker=reticle(which='rt'), ms=32, color="red")
    ax_full.coords.grid(True, color="white", alpha=0.35, ls=":")
    ax_full.set_xlabel("Right Ascension (J2000)")
    ax_full.set_ylabel("Declination (J2000)")
    ax_full.set_title(f"Full image @ t={times[t_full_center]:.3f}s (idx {t_full_center})")
    cbar1 = fig.colorbar(im_full, ax=ax_full, fraction=0.046, pad=0.02)
    cbar1.set_label("Flux Density (mJy/beam)")
    
    # Top-right: WCS cutout
    ax_cut = fig.add_subplot(gs[0,1], projection=wcs_cut)
    im_cut = ax_cut.imshow(snippet, origin="lower", cmap=cmap, vmin=vmin, vmax=vmax)
    ax_cut.plot([min(half_sp, x - x0)], [min(half_sp, y - y0)], marker=reticle(which='rt'), ms=32, color="red")
    ax_cut.coords.grid(True, color="white", alpha=0.35, ls=":")
    ax_cut.set_xlabel("Right Ascension (J2000)")
    ax_cut.set_ylabel("Declination (J2000)")
    ax_cut.set_title(f"Cutout")


    cbar2 = fig.colorbar(im_cut, ax=ax_cut, fraction=0.046, pad=0.02)
    cbar2.set_label("Flux density (mJy/beam)")    


    # Middle: full-res light curve
    ax_lc_full = fig.add_subplot(gs[1, :])  # spans full width below
    ax_lc_full.plot(times, lc_full, color="tab:blue", lw=1.6)
    ax_lc_full.axvline(times[t_full_center], color="red", ls="--", lw=1.0, label="Detection time")
    ax_lc_full.set_ylabel("Flux density (mJy/beam)")
    ax_lc_full.grid(True, alpha=0.3)
    ax_lc_full.legend(loc="best")

    # Bottom: boxcar-smoothed light curve
    ax_lc_box = fig.add_subplot(gs[2, :])
    if T_eff > 0:
        ax_lc_box.plot(times_sm, lc_sm, color="tab:green", lw=1.6)
        ax_lc_box.axvline(times_sm[k_center], color="red", ls="--", lw=1.0, label=f"Smoothed center (w={w})")
        ax_lc_box.legend(loc="best")
    else:
        ax_lc_box.text(0.5, 0.5, f"Width {w} > T; no smoothed LC",
                       transform=ax_lc_box.transAxes, ha="center", va="center", color="red")
    ax_lc_box.set_xlabel("Time (s)")
    ax_lc_box.set_ylabel(f"Flux density - boxcar mean, w={w} (mJy/beam)")
    ax_lc_box.grid(True, alpha=0.3)

    #fig.subplots_adjust(left=0.08, right=0.98, top=0.94, bottom=0.08, wspace=0.22, hspace=0.35)
    fig.savefig(out_fig, bbox_inches="tight")
    plt.close(fig)

    return {"lc_full": out_full, "lc_boxcar": out_box, "figure": out_fig}
    
def save_candidate_snippet_products(snippet_rec: dict,
                                    out_prefix: str,
                                    *,
                                    pixscale_rad: float,
                                    ra_rad: float,
                                    dec_rad: float,
                                    ra_sign: int = -1,
                                    dec_sign: int = 1,
                                    cmap: str = "gray",
                                    gif_fps: int = 5,
                                    dpi: int = 300) -> dict:
    """
    Save three products for a transient candidate snippet:
      1) animated GIF across time with RA/Dec labels (WCSAxes)
      2) static PNG at the detection frame + light curve of detection pixel
      3) FITS image of the detection frame with TAN WCS

    Parameters
    ----------
    snippet_rec : dict
        An entry from `extract_candidate_snippets(...)`:
        { "candidate": {...}, "snippet_cube": (T, N, N) array, "snippet_times": (T,) array, ... }
        The detection pixel is at the spatial center (N//2, N//2).
    out_prefix : str
        Prefix for output file names (e.g., '/path/run001_cand0001').
    pixscale_rad : float
        Pixel scale in radians per pixel (ducc wgridder input).
    ra_rad, dec_rad : float
        World coordinates (radians) of the detection pixel.
    ra_sign : int
        Sign for RA pixel scale in WCS (default -1 for standard leftward RA).
    cmap : str
        Matplotlib colormap for images.
    gif_fps : int
        Frames per second for GIF.
    dpi : int
        DPI for saved PNG/GIF frames.

    Returns
    -------
    dict with paths:
        { "gif": ..., "png": ..., "fits": ... }
    """
    cube = snippet_rec["snippet_cube"]  # (T, N, N)
    times = snippet_rec["snippet_times"]  # (T,)
    cand  = snippet_rec.get("candidate", {})
    T, N, M = cube.shape
    assert N == M, "snippet must be square"

    # Detection frame inside the snippet: the temporal center
    t_center = T // 2
    y_center = N // 2
    x_center = M // 2

    # Build WCS for RA/Dec labels

    wcs2d = ducc_wcs._build_tan_wcs_for_snippet(
        spatial_size=N,
        ra_rad=ra_rad,
        dec_rad=dec_rad,
        pixscale_rad=pixscale_rad,
        ra_sign=ra_sign,           # leftward RA increases (astronomical convention)
        dec_sign=dec_sign,
        radesys="ICRS",       # or "FK5" if that matches dataset
        equinox=None          # set 2000.0 if using FK5
    )
    
    # Robust display scaling
    det_frame = cube[t_center]
    vmin = np.nanpercentile(det_frame, 5.0)
    vmax = np.nanpercentile(det_frame, 99.5)

    out_gif  = f"{out_prefix}_snippet.gif"
    out_png  = f"{out_prefix}_det.png"
    out_fits = f"{out_prefix}_det.fits"

    # 1) Animated GIF with WCSAxes
    fig = plt.figure(figsize=(10, 10), dpi=dpi)
    ax = fig.add_subplot(111, projection=wcs2d)
    im = ax.imshow(cube[0], origin="lower", cmap=cmap, vmin=vmin, vmax=vmax)
    # Sky grid and labels
    ax.coords.grid(True, color="white", alpha=0.3, ls=":")
    ax.set_xlabel("Right Ascension (J2000)")
    ax.set_ylabel("Declination (J2000)")
    # Mark the detection pixel at center
    ax.plot([x_center], [y_center], marker=reticle(which='rt'), ms=32, color="red")
    # Title updates with time
    def _update(k: int):
        im.set_data(cube[k])
        t = times[k]
        if np.isfinite(t):
            ax.set_title(f"t = {t:.2f} s")
        else:
            ax.set_title("t = (pad)")
        return (im,)
    anim = FuncAnimation(fig, _update, frames=T, interval=1000//gif_fps, blit=False)
    anim.save(out_gif, writer=PillowWriter(fps=gif_fps))
    plt.close(fig)

    # 2) Static PNG at detection frame + light curve
    fig2 = plt.figure(figsize=(10, 10), dpi=dpi)
    gs = GridSpec(
        nrows=3, ncols=2, figure=fig,
        height_ratios=[1.0, 1.0, 1.0],   # give images slightly more height
        width_ratios=[1, 1],
    )
    
    # Top: WCSAxes image
    ax_img = fig2.add_subplot(gs[0:2, :], projection=wcs2d)
    im2 = ax_img.imshow(det_frame, origin="lower", cmap=cmap, vmin=vmin, vmax=vmax)
    ax_img.coords.grid(True, color="white", alpha=0.3, ls=":")
    ax_img.set_xlabel("Right Ascension (J2000)")
    ax_img.set_ylabel("Declination (J2000)")
    ax_img.plot([x_center], [y_center], marker=reticle(which='rt'), ms=32, color="red")
    cb = fig2.colorbar(im2, ax=ax_img, fraction=0.046, pad=0.04)
    cb.set_label("Intensity (arb. units)")
    # Title with RA/Dec, SNR if available
    ra_hms = cand.get("ra_hms", None)
    dec_dms = cand.get("dec_dms", None)
    title_txt = "Candidate frame"
    if ra_hms and dec_dms:
        title_txt += f"  |  RA={ra_hms}  Dec={dec_dms}"
    ax_img.set_title(title_txt)

    # Bottom: light curve of detection pixel
    ax_lc = fig2.add_subplot(gs[2, :])
    lc = cube[:, y_center, x_center]
    # Mask NaNs from pad zones
    good = np.isfinite(times) & np.isfinite(lc)
    ax_lc.plot(times[good], lc[good], color="tab:blue", lw=1.5)
    # Mark the detection center time
    t0 = times[t_center] if np.isfinite(times[t_center]) else np.nan

    print(cand)
    try:
        tline = cand.get("time_center_peak", cand["time_center"])
        ax_lc.axvline(tline, color="red", ls="--", lw=1.0, label="Candidate peak")

    except KeyError:
        print("time_center not found")
        
    # if np.isfinite(t0):
    #     ax_lc.axvline(t0, color="red", ls="--", lw=1.0, label="Detection time")
    #     ax_lc.legend(loc="best")
    ax_lc.set_xlabel("Time (s)")
    ax_lc.set_ylabel("Peak flux density (mJy/beam)")
    ax_lc.grid(True, alpha=0.3)

    fig2.savefig(out_png, bbox_inches="tight")
    plt.close(fig2)

    # 3) FITS image of detection frame with WCS header

    hdr = wcs2d.to_header()
    
    # Optional helpful metadata:
    hdr["BUNIT"] = "arb. unit"
    hdr["COMMENT"] = "Detection frame from transient snippet"
    if ra_hms and dec_dms:
        hdr["OBJRA"] = ra_hms
        hdr["OBJDEC"] = dec_dms
        
    # Ensure CDELT/PC are present (to_header() should already include them,
    # but we can set explicitly to be safe):
    # NOTE: when using CDELT+PC we DO NOT set CD.
    # CDELT1/2 in degrees per pixel
    hdr["CDELT1"] = wcs2d.wcs.cdelt[0]
    hdr["CDELT2"] = wcs2d.wcs.cdelt[1]
    
    # Identity PC
    hdr["PC1_1"] = 1.0; hdr["PC1_2"] = 0.0
    hdr["PC2_1"] = 0.0; hdr["PC2_2"] = 1.0
    
    # Frame metadata if present
    if getattr(wcs2d.wcs, "radesys", None):
        hdr["RADESYS"] = wcs2d.wcs.radesys
    #if getattr(wcs2d.wcs, "equinox", None) is not None:
    #    hdr["EQUINOX"] = wcs2d.wcs.equinox
        

    # CTYPE/CUNIT/CRPIX/CRVAL are already provided by to_header(), so no need to duplicate.
    
    hdu = fits.PrimaryHDU(data=det_frame.astype(np.float32), header=hdr)
    hdu.writeto(out_fits, overwrite=True)

    return {"gif": out_gif, "png": out_png, "fits": out_fits}


def _stack_tables_or_empty(tables):
    """Vstack a list of tables (outer join), return a filled table or empty schema."""
    if len(tables) == 0:
        # Create a minimal empty table with common columns used in your pipeline
        return Table(names=["x","y","l","m","ra_rad","dec_rad","ra_deg","dec_deg",
                            "ra_hms","dec_dms","snr","std","width_samples",
                            "time_start","time_end","time_center","t0_idx","t1_idx_excl",
                            "center_idx","phase_center_field","chunk_id","algo"],
                     dtype=[int,int,float,float,float,float,float,float,
                            "U20","U20",float,float,int,
                            float,float,float,int,int,
                            int,"U64",int,"U16"])
    # Use outer join to be resilient to column differences across chunks
    T = vstack(tables, join_type="outer", metadata_conflicts="silent")
    # Fill masked values (NA) with sensible defaults for CSV/VOT output
    T.fill_value = np.nan
    T = T.filled()
    return T


def _read_csv_tables(paths):
    """Read a list of CSV files into Astropy Tables (skip missing)."""
    tables = []
    for p in paths:
        try:
            t = Table.read(p, format="csv")
            tables.append(t)
        except Exception as e:
            print(f"[Consolidation] Skipping '{p}' (read error: {e})")
    return tables

def _stack_tables_or_empty(tables):
    """Vstack a list of tables (outer join), return a filled table or empty schema."""
    if len(tables) == 0:
        # Create a minimal empty table with common columns used in your pipeline
        return Table(names=["x","y","l","m","ra_rad","dec_rad","ra_deg","dec_deg",
                            "ra_hms","dec_dms","snr","std","width_samples",
                            "time_start","time_end","time_center","t0_idx","t1_idx_excl",
                            "center_idx","phase_center_field","chunk_id","algo"],
                     dtype=[int,int,float,float,float,float,float,float,
                            "U20","U20",float,float,int,
                            float,float,float,int,int,
                            int,"U64",int,"U16"])
    # Use outer join to be resilient to column differences across chunks
    T = vstack(tables, join_type="outer", metadata_conflicts="silent")
    # Fill masked values (NA) with sensible defaults for CSV/VOT output
    T.fill_value = np.nan
    T = T.filled()
    return T


def _load_psrcat_csv(psrcat_csv_path: str) -> Table:
    """
    Read ATNF PSRCAT exported CSV that is semicolon-separated and contains
    sexagesimal 'RAJ' (hms) and 'DECJ' (dms). Returns a Table with:
      ['src_name','ra_deg','dec_deg']
    Skips rows with missing RA/Dec.
    """
    # Read raw with Astropy (handles custom delimiters well)
    t = Table.read(psrcat_csv_path, format="ascii.csv", delimiter=";", guess=False, fast_reader=False)
    # Normalise column names that typically appear in this export
    # Expected: 'PSRJ','RAJ','DECJ' (sexagesimal strings)
    col_name = "PSRJ" if "PSRJ" in t.colnames else ("NAME" if "NAME" in t.colnames else None)
    raj = "RAJ" if "RAJ" in t.colnames else None
    decj = "DECJ" if "DECJ" in t.colnames else None
    if not (col_name and raj and decj):
        raise ValueError("PSRCAT CSV must contain PSRJ, RAJ, DECJ columns")

    # Build coords, skipping blanks
    names, ra_deg, dec_deg = [], [], []
    for r in t:
        ra_s = str(r[raj]).strip()
        dec_s = str(r[decj]).strip()
        if (not ra_s) or (ra_s in ("*", "nan")) or ((":" not in ra_s) and (" " not in ra_s)):
            continue
        if (not dec_s) or (dec_s in ("*", "nan")):
            continue
        try:
            c = SkyCoord(ra_s, dec_s, unit=(u.hourangle, u.deg), frame="icrs")
            names.append(str(r[col_name]).strip())
            ra_deg.append(float(c.ra.deg))
            dec_deg.append(float(c.dec.deg))
        except Exception:
            # Skip malformed line
            continue

    return Table(
        {
            "src_name": names,
            "ra_deg": ra_deg,
            "dec_deg": dec_deg,
        }
    )

def _load_racs_votable(racs_vot_path: str) -> Table:
    """
    Read RACS components VOTable and normalise to ['src_name','ra_deg','dec_deg'].
    Uses 'Source_ID' if present, else 'Gaussian_ID'. Coordinates from RA/Dec (deg).
    """
    t = Table.read(racs_vot_path, format="votable")
    # Choose ID column
    id_col = "Name" #  Source_ID" if "Source_ID" in t.colnames else ("Gaussian_ID" if "Gaussian_ID" in t.colnames else None)
    # if id_col is None:
    #     id_col = "ID" if "ID" in t.colnames else None
    # if id_col is None:
    #     id_col = "name" if "name" in t.colnames else None
    # if id_col is None:
    #     raise ValueError("Could not find an identifier column in RACS VOTable (e.g. 'Source_ID' or 'Gaussian_ID').")

    # Coordinates
    if not (("RA" in t.colnames) and ("Dec" in t.colnames)):
        raise ValueError("RACS VOTable must contain 'RA' and 'Dec' columns (degrees).")

    return Table(
        {
            "src_name": [str(x) for x in t[id_col]],
            "ra_deg":   [float(x) for x in t["RA"]],
            "dec_deg":  [float(x) for x in t["Dec"]],
        }
    )

def _nearest_within_radius(target: SkyCoord, cat: SkyCoord, radius: u.Quantity) -> tuple:
    """
    Return (index, separation) of the nearest neighbour within radius.
    If none within radius: return (-1, None).
    """
    if len(cat) == 0:
        return -1, None
    sep = target.separation(cat)
    j = int(sep.argmin())
    if sep[j] <= radius:
        return j, sep[j]
    return -1, None


def parse_candidate_filename(
    path: str,
    *,
    require: str | None = None,     # None | 'all' | 'super'
) -> dict:
    """
    Parse pipeline candidate/super-summary filenames.

    Supports both styles:
      (A) Per-beam consolidated "all" tables, e.g.
          cracoData.<field>.SB77974.beam14.20251015072402.<...>_<kind>_all.(csv|vot)
      (B) Per-scan super-summary tables, e.g.
          <field>.<SBID>_<scan_id>_<kind>_super_summary.(vot|csv)

    Returns a dict with keys:
      {
        'field': str,
        'sbid': str,
        'beam': str,      # '' if absent
        'scan_id': str,   # '' if absent
        'kind': str,      # 'boxcar' | 'variance' | ''
        'is_all': bool,       # True if "*_all.*"
        'is_super': bool,     # True if "*_super_summary.*"
      }

    If `require='all'` or `require='super'` is set and the filename does not
    match that shape, returns an "empty" dict with the same keys (all falsy).
    """
    fname = os.path.basename(path)

    # Initialize with empty defaults
    info = {
        "field": "",
        "sbid": "",
        "beam": "",
        "scan_id": "",
        "kind": "",
        "is_all": False,
        "is_super": False,
    }

    # --- Field ---
    # Try "<field>." prefix (e.g. "LTR_1733-2344.SB..."), else fall back to explicit LTR pattern.
    m_field_prefix = re.match(r'^(?P<field>[^.]+)\.', fname)
    if m_field_prefix:
        info["field"] = m_field_prefix.group("field")
    else:
        m_field_alt = re.search(r'(LTR_\d{4}(?:\+/-|[+-])\d{4})', fname)
        if m_field_alt:
            info["field"] = m_field_alt.group(1)

    # --- SBID ---
    m_sbid = re.search(r'(SB\d{5,})', fname)
    if m_sbid:
        info["sbid"] = m_sbid.group(1)

    # --- Beam (optional) ---
    m_beam = re.search(r'(beam\d+)', fname)
    if m_beam:
        info["beam"] = m_beam.group(1)

    # --- Scan id (14-digit timestamp) ---
    m_scan = re.search(r'(\d{14})', fname)
    if m_scan:
        info["scan_id"] = m_scan.group(1)

    # --- Kind and shape ("all" vs "super_summary") ---
    m_kind_super = re.search(r'_(boxcar|variance)_super_summary\.(?:vot|xml|csv)$', fname, re.IGNORECASE)
    m_kind_all   = re.search(r'_(boxcar|variance)_all\.(?:vot|xml|csv)$', fname, re.IGNORECASE)

    if m_kind_super:
        info["kind"] = m_kind_super.group(1).lower()
        info["is_super"] = True
    elif m_kind_all:
        info["kind"] = m_kind_all.group(1).lower()
        info["is_all"] = True

    # --- Optional strictness ---
    if require == "super" and not info["is_super"]:
        return {k: (False if isinstance(v, bool) else "") for k, v in info.items()}
    if require == "all" and not info["is_all"]:
        return {k: (False if isinstance(v, bool) else "") for k, v in info.items()}

    return info

        
def _parse_field_sbid_scan_from_obs_super_path(path: str) -> tuple[str, str, str, str]:
    """
    Extract (field, SBID, scan_id, kind) from a per-scan super-summary filename:
      <field>.<SBID>_<scan_id>_<kind>_super_summary.vot
    Example:
      LTR_1733-2344.SB77974_20251015091053_variance_super_summary.vot
    Returns ('', '', '', '') on failure.
    """
    fname = os.path.basename(path)
    field = ''
    sbid = ''
    scan = ''
    kind = ''

    # field: allow e.g. LTR_1733-2344
    m_field = re.match(r'^(?P<field>[^.]+)\.', fname)
    if m_field:
        field = m_field.group('field')

    m_sbid = re.search(r'(SB\d{5})', fname)
    if m_sbid:
        sbid = m_sbid.group(1)

    m_scan = re.search(r'(\d{14})', fname)
    if m_scan:
        scan = m_scan.group(1)

    m_kind = re.search(r'_(boxcar|variance)_super_summary\.(?:vot|xml|VOT|XML|csv)$', fname)
    if m_kind:
        kind = m_kind.group(1)

    return field, sbid, scan, kind

         
def _parse_beam_and_scan_from_filename(path: str) -> tuple[str, str, str, str]:
    """Extract beam id (e.g. 'beam14') and scan id (e.g. '20251015072402') from a filename.

    Expected filename style (dot-separated tokens plus suffix), for example:
        cracoData.<field>.SB77974.beam14.20251015072402.<...>_boxcar_all.csv

    Returns
    -------
    (beam_id, scan_id) : tuple[str, str]
        Empty strings if not found.
    """
    fname = os.path.basename(path)
    fieldname = ''
    beam = ''
    scan = ''
    m_field = re.search(r'(LTR_\d{4}(?:\+/-|-)\d{4})', fname)
    if m_field:
        field = m_field.group(1)
    m_sbid = re.search(r'(SB\d{5})', fname)
    if m_sbid:
        sbid = m_sbid.group(1)
    m_beam = re.search(r'(beam\d+)', fname)
    if m_beam:
        beam = m_beam.group(1)
    m_scan = re.search(r'(\d{14})', fname)
    if m_scan:
        scan = m_scan.group(1)
    
    return field, sbid, beam, scan


def _connected_components_from_pairs(n: int, i_idx, j_idx) -> List[List[int]]:
    """Compute connected components from undirected edge lists.

    Parameters
    ----------
    n : int
        Number of nodes.
    i_idx, j_idx : iterable[int]
        Edge endpoints (same-length iterables). Self-edges are ignored.

    Returns
    -------
    comps : list[list[int]]
        Each component is a list of node indices.
    """
    adj: List[List[int]] = [[] for _ in range(n)]
    for a, b in zip(i_idx, j_idx):
        a = int(a)
        b = int(b)
        if a == b:
            continue
        adj[a].append(b)
        adj[b].append(a)

    seen = [False] * n
    comps: List[List[int]] = []
    for s in range(n):
        if seen[s]:
            continue
        stack = [s]
        seen[s] = True
        comp = [s]
        while stack:
            v = stack.pop()
            for w in adj[v]:
                if not seen[w]:
                    seen[w] = True
                    stack.append(w)
                    comp.append(w)
        comps.append(comp)
    return comps


def _time_groups(times: np.ndarray, tol_s: float) -> List[Tuple[int, int]]:
    """Return [(i0,i1), ...] contiguous groups where (times[i]-times[i0]) <= tol_s."""
    if times.size == 0:
        return []
    order = np.argsort(times)
    times_sorted = times[order]
    groups = []
    i0 = 0
    N = times_sorted.size
    while i0 < N:
        t0 = times_sorted[i0]
        i1 = i0 + 1
        while i1 < N and (times_sorted[i1] - t0) <= tol_s:
            i1 += 1
        # Map group back to original indices
        idxs = order[i0:i1]
        # Return as a compact (min..max+1) slice if contiguous after sorting
        groups.append((int(idxs.min()), int(idxs.max()) + 1))
        i0 = i1
    return groups

def _cluster_indices_by_sky(ra_deg: np.ndarray,
                            dec_deg: np.ndarray,
                            seplimit_arcsec: float) -> List[List[int]]:
    """
    Return list of components (each a list of row indices) by clustering positions
    within seplimit_arcsec.
    """
    coords = SkyCoord(ra=ra_deg * u.deg, dec=dec_deg * u.deg, frame="icrs")
    i_idx, j_idx, _, _ = coords.search_around_sky(coords, seplimit=seplimit_arcsec * u.arcsec)
    comps = _connected_components_from_pairs(len(ra_deg), i_idx, j_idx)
    return comps

def _sexagesimal_from_deg(ra_deg: float, dec_deg: float) -> Tuple[str, str, str]:
    """(ra_hms, dec_dms, srcname) from degrees using ducc_wcs helpers."""
    ra_rad = math.radians(float(ra_deg))
    dec_rad = math.radians(float(dec_deg))
    ra_hms, dec_dms = ducc_wcs.rad_to_hmsdms(ra_rad, dec_rad, dp=1)
    srcname = ducc_wcs.hmsdms_to_srcname(ra_hms, dec_dms)
    return ra_hms, dec_dms, srcname

def _ensure_sexagesimal_on_rows(rows: List[Dict[str, Any]],
                                ra_key: str = "ra_deg",
                                dec_key: str = "dec_deg",
                                ra_hms_key: str = "ra_hms",
                                dec_dms_key: str = "dec_dms",
                                src_key: str = "srcname") -> None:
    """
    Ensure each dict row has ra_hms, dec_dms, and srcname. Fill if missing/blank.
    """
    for r in rows:
        have_ra = r.get(ra_key, None) is not None
        have_dec = r.get(dec_key, None) is not None
        if not have_ra or not have_dec:
            continue
        ra_hms = str(r.get(ra_hms_key, "") or "")
        dec_dms = str(r.get(dec_dms_key, "") or "")
        srcname = str(r.get(src_key, "") or "")
        if ra_hms and dec_dms and srcname:
            continue
        ra_hms_new, dec_dms_new, srcname_new = _sexagesimal_from_deg(float(r[ra_key]), float(r[dec_key]))
        if not ra_hms:
            r[ra_hms_key] = ra_hms_new
        if not dec_dms:
            r[dec_dms_key] = dec_dms_new
        if not srcname:
            r[src_key] = srcname_new


def _split_list_str(s: Any) -> List[str]:
    """Parse comma-separated string -> list[str]."""
    if s is None:
        return []
    s = str(s).strip().strip('"')
    if not s:
        return []
    return [t.strip() for t in s.split(",") if t.strip()]


def _split_list_int(s: Any) -> List[int]:
    """Parse comma-separated string -> list[int] (skip non-ints)."""
    out = []
    for tok in _split_list_str(s):
        try:
            out.append(int(tok))
        except Exception:
            pass
    return out



def consolidate_chunk_catalogues(
    *,
    ms_base: str,                         # e.g., "example" from "example.ms"
    out_dir: str,                         # e.g., "<MS parent>/candidates"
    var_csv_pattern: str,                 # e.g., "<candidates>/<ms_base>_chunk_*_var_candidates.csv"
    box_csv_pattern: str,                 # e.g., "<candidates>/<ms_base>_chunk_*_boxcar_candidates.csv"
    remove_chunk_catalogues: bool = True
):
    """
    Consolidate per-chunk catalogues from disk into MS-local `candidates/`,
    write consolidated CSV+VOT with names like:
        <out_dir>/<ms_base>_variance_all.csv
        <out_dir>/<ms_base>_variance_all.vot
        <out_dir>/<ms_base>_boxcar_all.csv
        <out_dir>/<ms_base>_boxcar_all.vot
    Optionally remove per-chunk CSV/VOT files afterwards.

    Parameters
    ----------
    ms_base : str
        Base name of the measurement set (e.g., "example" for "example.ms").
    out_dir : str
        Destination directory (the `candidates/` folder next to the MS).
    var_csv_pattern : str
        Glob pattern for variance per-chunk CSV files.
        Example: os.path.join(candidates_dir, f"{ms_base}_chunk_*_var_candidates.csv")
    box_csv_pattern : str
        Glob pattern for boxcar per-chunk CSV files.
        Example: os.path.join(candidates_dir, f"{ms_base}_chunk_*_boxcar_candidates.csv")
    remove_chunk_catalogues : bool
        If True, delete per-chunk CSV/VOT files after consolidation.

    Returns
    -------
    None
    """
    os.makedirs(out_dir, exist_ok=True)

    # 1) Glob per-chunk CSV files
    var_csv_files = sorted(glob.glob(var_csv_pattern))
    box_csv_files = sorted(glob.glob(box_csv_pattern))

    print(f"[Consolidation] Found {len(var_csv_files)} variance CSVs and {len(box_csv_files)} boxcar CSVs.")

    # 2) Read and stack
    var_tables = _read_csv_tables(var_csv_files)
    box_tables = _read_csv_tables(box_csv_files)

    t_var_all = _stack_tables_or_empty(var_tables)
    t_box_all = _stack_tables_or_empty(box_tables)

    # 3) Write consolidated catalogues (CSV + VOT) into candidates dir with ms_base prefix
    var_out_csv = os.path.join(out_dir, f"{ms_base}_variance_all.csv")
    var_out_vot = os.path.join(out_dir, f"{ms_base}_variance_all.vot")
    t_var_all.write(var_out_csv, format="csv", overwrite=True)
    t_var_all.write(var_out_vot, format="votable", overwrite=True)

    box_out_csv = os.path.join(out_dir, f"{ms_base}_boxcar_all.csv")
    box_out_vot = os.path.join(out_dir, f"{ms_base}_boxcar_all.vot")
    t_box_all.write(box_out_csv, format="csv", overwrite=True)
    t_box_all.write(box_out_vot, format="votable", overwrite=True)

    print(f"[Consolidation] Wrote {len(t_var_all)} variance candidates -> {var_out_csv}, {var_out_vot}")
    print(f"[Consolidation] Wrote {len(t_box_all)} boxcar candidates   -> {box_out_csv}, {box_out_vot}")

    # 4) Remove per-chunk catalogues (CSV + VOT) if requested
    if remove_chunk_catalogues:
        var_vot_files = sorted(glob.glob(var_csv_pattern.replace(".csv", ".vot")))
        box_vot_files = sorted(glob.glob(box_csv_pattern.replace(".csv", ".vot")))
        removed = 0
        for p in (var_csv_files + var_vot_files + box_csv_files + box_vot_files):
            try:
                os.remove(p)
                removed += 1
            except Exception as e:
                print(f"[Consolidation] Could not remove '{p}': {e}")
        print(f"[Consolidation] Removed {removed} per-chunk catalogue files.")

        
def aggregate_beam_candidate_tables(
    vot_files: List[str],
    *,
    kind: str,
    time_tol_s: float = 0.3,
    sky_tol_arcsec: float = 35.0,
    out_dir: Optional[str] = None,
) -> Table:
    """
    Crossmatch candidates across beams within each scan and build a field-level summary,
    then produce a per-scan super summary by clustering events by sky (time-marginalised).

    This is intentionally lightweight:
      * Time grouping: detections are sorted by time_center and split into groups
        whose span is <= time_tol_s (default 0.3 s).
      * Spatial crossmatch within each time group uses Astropy's spherical search
        (KD-tree accelerated), via SkyCoord.search_around_sky.
      * Clustering: connected components over the returned neighbor pairs.
      * Summary: the max-SNR detection in each cluster defines the reported
        time_center, ra_deg, dec_deg, and max_snr.

    Parameters
    ----------
    vot_files : list[str]
        List of per-beam consolidated candidate VOTs (e.g. '*_boxcar_all.vot'
        or '*_variance_all.vot').
    kind : {'boxcar','variance'}
        Candidate type. If 'boxcar', widths are aggregated.
    time_tol_s : float
        Time tolerance (seconds) for grouping.
    sky_tol_arcsec : float
        Spatial tolerance (arcsec) for crossmatch.

    Returns
    -------
    astropy.table.Table
        One row per crossmatched event.    
    """
    if not vot_files:
        raise ValueError("No input files provided")

    obs_root = os.path.abspath(os.path.dirname(vot_files[0]))
    if out_dir is None:
        out_dir = obs_root
    os.makedirs(out_dir, exist_ok=True)

    kind = kind.lower().strip()
    if kind not in ("boxcar", "variance"):
        raise ValueError("kind must be 'boxcar' or 'variance'")

    # Determine naming from the first readable file
    out_csv = None
    out_vot = None
    for p in vot_files:
        meta = parse_candidate_filename(p, require="all")
        field_name = meta["field"]
        sbid = meta["sbid"]
        scan_id = meta["scan_id"]
        if field_name and sbid and scan_id:
            out_csv = os.path.join(out_dir, f"{field_name}.{sbid}_{scan_id}_{kind}_summary.csv")
            out_vot = os.path.join(out_dir, f"{field_name}.{sbid}_{scan_id}_{kind}_summary.vot")
            break

    # -------- Ingest all per-beam detections into a flat row list ----------
    rows: List[Dict[str, Any]] = []
    for p in vot_files:
        try:
            t = Table.read(p, format="votable")
        except Exception as e:
            print(f"[Aggregate] Skipping '{p}' (read error: {e})")
            continue
        meta = parse_candidate_filename(p, require="all")
        field_name = meta["field"]; sbid = meta["sbid"]; beam_id = meta["beam"]; scan_id = meta["scan_id"]
        for col in ("time_center", "ra_deg", "dec_deg", "snr"):
            if col not in t.colnames:
                raise ValueError(f"Missing required column '{col}' in {p}")
        for r in t:
            d = {name: r[name] for name in t.colnames}
            d["beam_id"] = beam_id
            d["scan_id"] = scan_id
            d["field"]   = field_name
            d["sbid"]    = sbid
            rows.append(d)

    # Empty case
    if len(rows) == 0:
        names = ['event_id', 'time_center', 'ra_deg', 'dec_deg', 'max_snr', 'beam_ids', 'field', 'sbid']
        if kind == 'boxcar':
            names.append('width_samples')
        names += ['scan_ids', 'n_beams', 'n_detections']
        out = Table(names=names)
        if out_csv: out.write(out_csv, format='csv', overwrite=True)
        if out_vot: out.write(out_vot, format='votable', overwrite=True)
        return out

    # ------------------- Event-level aggregation -------------------
    # Group by time first (<= time_tol_s from first in group), then cluster by sky.
    times = np.array([float(r['time_center']) for r in rows], dtype=np.float64)
    # Use stable order indices for slicing; we will build groups as index lists
    order = np.argsort(times)
    rows = [rows[i] for i in order]
    times = times[order]

    # Build the (i0,i1) groups over the *sorted* rows
    groups = []
    i0 = 0
    N = len(rows)
    while i0 < N:
        t0 = times[i0]
        i1 = i0 + 1
        while i1 < N and (times[i1] - t0) <= time_tol_s:
            i1 += 1
        groups.append((i0, i1))
        i0 = i1

    events: List[Dict[str, Any]] = []
    for (a0, a1) in groups:
        k = a1 - a0
        if k <= 0:
            continue
        if k == 1:
            det = rows[a0]
            beams = sorted({det.get('beam_id', '')})
            scans = sorted({det.get('scan_id', '')})
            ra_hms, dec_dms, srcname = _sexagesimal_from_deg(float(det["ra_deg"]), float(det["dec_deg"]))
            ev = {
                'srcname': srcname,
                'time_center': float(det['time_center']),
                'ra_deg': float(det['ra_deg']),
                'dec_deg': float(det['dec_deg']),
                'ra_hms': str(ra_hms),
                'dec_dms': str(dec_dms),
                'max_snr': float(det['snr']),
                'beam_ids': ','.join([b for b in beams if b]),
                'scan_ids': ','.join([s for s in scans if s]),
                'fields': det.get('field', ''),
                'sbids' : det.get('sbid', ''),
                'n_beams': len([b for b in beams if b]),
                'n_detections': 1,
            }
            if kind == 'boxcar':
                w = det.get('width_samples', None)
                ev['width_samples'] = str(int(w)) if w is not None else ''
            events.append(ev)
            continue

        # Spatial clustering within the time group
        ra = np.array([float(rows[a0+i]['ra_deg']) for i in range(k)], dtype=float)
        dec = np.array([float(rows[a0+i]['dec_deg']) for i in range(k)], dtype=float)
        comps = _cluster_indices_by_sky(ra, dec, seplimit_arcsec=sky_tol_arcsec)
        for comp in comps:
            inds = [a0 + int(ii) for ii in comp]
            sub = [rows[ii] for ii in inds]
            best = max(sub, key=lambda r: float(r.get('snr', -np.inf)))
            beams = sorted({r.get('beam_id', '') for r in sub})
            scans = sorted({r.get('scan_id', '') for r in sub})
            ra_hms, dec_dms, srcname = _sexagesimal_from_deg(float(best["ra_deg"]), float(best["dec_deg"]))
            ev = {
                'srcname': srcname,
                'time_center': float(best['time_center']),
                'ra_deg': float(best['ra_deg']),
                'dec_deg': float(best['dec_deg']),
                'ra_hms': str(ra_hms),
                'dec_dms': str(dec_dms),
                'max_snr': float(best['snr']),
                'beam_ids': ','.join([b for b in beams if b]),
                'scan_ids': ','.join([s for s in scans if s]),
                'fields': ','.join(sorted({r.get('field','') for r in sub})),
                'sbids' : ','.join(sorted({r.get('sbid','') for r in sub})),
                'n_beams': len([b for b in beams if b]),
                'n_detections': len(sub),
            }
            if kind == 'boxcar':
                widths = sorted({int(r.get('width_samples', -1)) for r in sub
                                 if r.get('width_samples', None) is not None and int(r.get('width_samples', -1)) >= 0})
                ev['width_samples'] = ','.join([str(w) for w in widths])
            events.append(ev)

    events.sort(key=lambda r: r['time_center'])
    for i, ev in enumerate(events):
        ev['event_id'] = i

    # Build event table (columns preserved)
    colnames = ['event_id', 'srcname', 'time_center', 'ra_deg', 'dec_deg', 'ra_hms', 'dec_dms',
                'max_snr', 'beam_ids']
    if kind == 'boxcar':
        colnames.append('width_samples')
    colnames += ['scan_ids', 'n_beams', 'n_detections']
    event_tab = Table(rows=[[ev.get(c) for c in colnames] for ev in events], names=colnames)
    if out_csv: event_tab.write(out_csv, format='csv', overwrite=True)
    if out_vot: event_tab.write(out_vot, format='votable', overwrite=True)

    # ------------------- Super-summary within scan -------------------
    # Reuse sky clustering on the *event* table (time-marginalised).
    # Ensure sexagesimal/srcname present
    event_rows = [dict(zip(colnames, row)) for row in event_tab.as_array()]
    _ensure_sexagesimal_on_rows(event_rows, ra_key="ra_deg", dec_key="dec_deg",
                                ra_hms_key="ra_hms", dec_dms_key="dec_dms", src_key="srcname")

    ra_all = np.array([float(r["ra_deg"]) for r in event_rows])
    dec_all = np.array([float(r["dec_deg"]) for r in event_rows])
    comps = _cluster_indices_by_sky(ra_all, dec_all, seplimit_arcsec=sky_tol_arcsec)

    super_rows = []
    for comp in comps:
        sub = [event_rows[i] for i in comp]
        # max-SNR representative
        best = max(sub, key=lambda r: float(r.get("max_snr", -np.inf)))
        beams_union = set()
        scans_union = set()
        for r in sub:
            beams_union |= set(_split_list_str(r.get("beam_ids","")))
            scans_union |= set(_split_list_str(r.get("scan_ids","")))
        row = {
            "srcname": best["srcname"],
            "ra_deg": float(best["ra_deg"]),
            "dec_deg": float(best["dec_deg"]),
            "ra_hms": best["ra_hms"],
            "dec_dms": best["dec_dms"],
            "n_events": len(sub),
            "n_detections_total": int(np.nansum([float(r.get("n_detections", 1)) for r in sub])),
            "beams_all": ",".join(sorted([b for b in beams_union if b])),
            "scans_all": ",".join(sorted([s for s in scans_union if s])),
            "max_snr": float(best["max_snr"]),
            "max_snr_time_center": float(best.get("time_center", np.nan)),
            "max_snr_event_beams": best.get("beam_ids", ""),
        }
        if kind == "boxcar":
            w_union = set()
            for r in sub:
                w_union |= set(_split_list_int(r.get("width_samples","")))
            row["width_samples_all"] = ",".join(str(w) for w in sorted(w_union))
            # Try infer an unambiguous width for max-SNR if possible
            ws_best = _split_list_int(best.get("width_samples",""))
            row["max_snr_width"] = (ws_best[0] if len(ws_best) == 1 else -1)
            # Optional: a single beam if unambiguous
            beams_best = _split_list_str(best.get("beam_ids",""))
            row["max_snr_beam"] = (beams_best[0] if len(beams_best) == 1 else "")
        else:
            beams_best = _split_list_str(best.get("beam_ids",""))
            row["max_snr_beam"] = (beams_best[0] if len(beams_best) == 1 else "")

        super_rows.append(row)

    super_rows.sort(key=lambda r: float(r.get("max_snr", -np.inf)), reverse=True)
    for i, r in enumerate(super_rows):
        r["source_id"] = i

    super_cols = [
        "source_id","srcname","ra_deg","dec_deg","ra_hms","dec_dms",
        "n_events","n_detections_total","beams_all","scans_all",
        "max_snr","max_snr_time_center","max_snr_event_beams",
    ]
    if kind == "boxcar":
        super_cols += ["width_samples_all", "max_snr_width", "max_snr_beam"]
    else:
        super_cols += ["max_snr_beam"]

    super_tab = Table(rows=[[r.get(c) for c in super_cols] for r in super_rows], names=super_cols)

    # Persist super summary next to event summary
    try:
        super_csv = out_csv.replace("_summary.csv", "_super_summary.csv") if out_csv else None
        super_vot = out_vot.replace("_summary.vot", "_super_summary.vot") if out_vot else None
    except Exception:
        super_csv = None; super_vot = None
    if not super_csv:
        super_csv = os.path.join(out_dir, f"field_{kind}_super_summary.csv")
    if not super_vot:
        super_vot = os.path.join(out_dir, f"field_{kind}_super_summary.vot")
    super_tab.write(super_csv, format="csv", overwrite=True)
    super_tab.write(super_vot, format="votable", overwrite=True)

    return event_tab



def aggregate_observation(
    obs_root: str,
    *,
    kind: str = 'boxcar',
    time_tol_s: float = 0.3,
    sky_tol_arcsec: float = 35.0,
    out_dir: Optional[str] = None,
    pattern: Optional[str] = None,
) -> Table:
    """Discover per-beam candidate tables under obs_root and write a field-level summary."""
    kind = kind.lower().strip()
    if pattern is None:
        suf = '*_boxcar_all.vot' if kind == 'boxcar' else '*_variance_all.vot'
        pattern = os.path.join(obs_root, '**', 'candidates', suf) #obs_root/candidates/*.vot
        

    files = sorted(glob.glob(pattern, recursive=True))

    if out_dir is None:
        out_dir = obs_root
    os.makedirs(out_dir, exist_ok=True)

    # out_csv = os.path.join(out_dir, f"field_{kind}_summary.csv")
    # out_vot = os.path.join(out_dir, f"field_{kind}_summary.vot")
    print(f"[Aggregate] Found {len(files)} '{kind}' tables under '{obs_root}'")

    return aggregate_beam_candidate_tables(
        files,
        kind=kind,
        time_tol_s=time_tol_s,
        sky_tol_arcsec=sky_tol_arcsec,
        out_dir=out_dir,
        # out_csv=out_csv,
        # out_vot=out_vot,
    )



def annotate_observation_with_catalogs(
    out_tab: Table,
    *,
    psrcat_csv_path: str = None,
    racs_vot_path: str = None,
    match_radius_arcsec: float = 40.0,
    simbad_enable: bool = True,
) -> Table:
    """
    Enrich an observation-level candidate table with three new columns:
      - 'source_name'           (str)    : matched name or 'unknown'
      - 'source_offset_arcsec'  (float)  : match separation in arcsec (NaN if unknown)
      - 'source_catalog'        (str)    : 'PSRCAT' | 'RACS' | 'SIMBAD' | ''
    Priority: PSRCAT -> RACS -> SIMBAD (if still unmatched).
    """
    # Guard: the observation table must carry ra/dec in degrees.
    if not (("ra_deg" in out_tab.colnames) and ("dec_deg" in out_tab.colnames)):
        raise ValueError("Expected 'ra_deg' and 'dec_deg' columns in observation table")

    # Prepare new columns with defaults
    source_name = ["unknown"] * len(out_tab)
    source_offset = [float("nan")] * len(out_tab)
    source_cat = [""] * len(out_tab)

    # Build candidate coords
    cand_coord = SkyCoord(out_tab["ra_deg"], out_tab["dec_deg"], unit="deg", frame="icrs")
    radius = match_radius_arcsec * u.arcsec

    # --- 1) PSRCAT (optional) ---
    psrcat_coord = None
    psrcat_tbl = None
    if psrcat_csv_path:
        psrcat_tbl = _load_psrcat_csv(psrcat_csv_path)
        psrcat_coord = SkyCoord(psrcat_tbl["ra_deg"], psrcat_tbl["dec_deg"], unit="deg", frame="icrs")

    # --- 2) RACS (optional) ---
    racs_coord = None
    racs_tbl = None
    if racs_vot_path:
        racs_tbl = _load_racs_votable(racs_vot_path)
        racs_coord = SkyCoord(racs_tbl["ra_deg"], racs_tbl["dec_deg"], unit="deg", frame="icrs")

    # Iterate candidates (few hundred -> simple loop OK; Astropy uses vectorised internals)
    for i in range(len(out_tab)):
        tcoord = cand_coord[i]

        # PSRCAT first
        if psrcat_coord is not None and len(psrcat_coord) > 0:
            j, sep = _nearest_within_radius(tcoord, psrcat_coord, radius)
            if j >= 0:
                source_name[i] = str(psrcat_tbl["src_name"][j])
                source_offset[i] = float(sep.to(u.arcsec).value)
                source_cat[i] = "PSRCAT"
                continue

        # Then RACS
        if racs_coord is not None and len(racs_coord) > 0:
            j, sep = _nearest_within_radius(tcoord, racs_coord, radius)
            if j >= 0:
                source_name[i] = str(racs_tbl["src_name"][j])
                source_offset[i] = float(sep.to(u.arcsec).value)
                source_cat[i] = "RACS"
                continue

        # Finally SIMBAD (network; optional)
        if simbad_enable:
            try:
                from astroquery.simbad import Simbad
                # Smaller radius for SIMBAD is fine
                # keep it equal to match_radius for now
                result = Simbad.query_region(tcoord, radius=radius)
                if result is not None and len(result) > 0:
                    # Choose nearest among returned rows
                    # SIMBAD returns RA, DEC as sexagesimal strings by default
                    # e.g., RA: '12 34 56.7', DEC: '+12 34 56'
                    # Build coords and take nearest
                    sc_list = []
                    for r in result:
                        try:
                            sc = SkyCoord(str(r["RA"]), str(r["DEC"]),
                                          unit=(u.hourangle, u.deg), frame="icrs")
                            sc_list.append(sc)
                        except Exception:
                            continue
                    if sc_list:
                        cat = SkyCoord([c.ra.deg for c in sc_list],
                                       [c.dec.deg for c in sc_list], unit="deg", frame="icrs")
                        j, sep = _nearest_within_radius(tcoord, cat, radius)
                        if j >= 0:
                            # Prefer MAIN_ID if present, else fallback to first column
                            main_id = str(result["MAIN_ID"][j]) if "MAIN_ID" in result.colnames else str(result[0][j])
                            source_name[i] = main_id
                            source_offset[i] = float(sep.to(u.arcsec).value)
                            source_cat[i] = "SIMBAD"
                            continue
            except Exception:
                # astroquery might be unavailable or offline; in that case, keep 'unknown'
                pass

    # Inject columns next to the name columns if present, else append at the end
    # By default, place right after 'srcname' if it exists
    insert_at = len(out_tab.colnames)
    if "srcname" in out_tab.colnames:
        insert_at = out_tab.colnames.index("srcname") + 1

    out_tab.add_column(Table.Column(name="source_name", data=source_name), index=insert_at)
    out_tab.add_column(Table.Column(name="source_offset_arcsec", data=source_offset), index=insert_at + 1)
    out_tab.add_column(Table.Column(name="source_catalog", data=source_cat), index=insert_at + 2)

    return out_tab



def aggregate_observation_from_super_summaries(
    obs_root: str,
    *,
    kind: str = "variance",
    sky_tol_arcsec: float = 35.0,
    out_dir: Optional[str] = None,
    pattern: Optional[str] = None,
    psrcat_csv_path: Optional[str] = None,   # e.g. "/path/to/psrcat_south.csv"
    racs_vot_path: Optional[str] = None,     # e.g. "/path/to/RACS-mid1_components.xml"
    match_radius_arcsec: float = 40.0,
    simbad_enable: bool = False,
) -> Table:    
    """
    Read *per-scan* cross-beam super summaries (VOT) under obs_root,
    cluster by sky position across all scans, and write an *observation-level*
    (SBID-wide) super summary.    

    reuses shared helpers for sky clustering, list parsing, and sexagesimal filling.

    Inputs
    ------
    obs_root : str
        Path to SBID directory, e.g. ".../SB77974".
        Expects subdirs like "SB77974/<scanid>/candidates/...".
    kind : {'variance','boxcar'}
        Which candidate family to ingest. Default: 'variance'.
    sky_tol_arcsec : float
        Spatial clustering tolerance across scans. Default: 35 arcsec.
    out_dir : str or None
        Where to write the obs-level outputs. Default: "<obs_root>/candidates".
    pattern : str or None
        Optional glob override. By default:
          obs_root/**/candidates/*_<kind>_super_summary.vot

    Outputs
    -------
    Writes two files:
      <out_dir>/<field>.<SBID>_obs_<kind>_super_summary.csv
      <out_dir>/<field>.<SBID>_obs_<kind>_super_summary.vot

    Returns
    -------
    astropy.table.Table
        The observation-level super summary table.    
    """
    kind = kind.lower().strip()
    if kind not in ("variance", "boxcar"):
        raise ValueError("kind must be 'variance' or 'boxcar'")

    if pattern is None:
        pattern = os.path.join(obs_root, "**", "candidates", f"*_{kind}_super_summary.vot")
    files = sorted(glob.glob(pattern, recursive=True))

    if out_dir is None:
        out_dir = os.path.join(obs_root, "candidates")
    os.makedirs(out_dir, exist_ok=True)

    # Empty inputs -> write empty outputs
    if len(files) == 0:
        empty_cols = [
            "source_id","srcname","ra_deg","dec_deg","ra_hms","dec_dms",
            "max_snr","max_snr_time_center","max_snr_beam","n_scans",
            "scan_ids","beams_all",
        ]
        if kind == "boxcar":
            empty_cols += ["max_snr_width","width_samples_all"]
        T = Table(names=empty_cols)
        field = "field"
        sbid = os.path.basename(os.path.abspath(obs_root))
        csv_out = os.path.join(out_dir, f"{field}.{sbid}_obs_{kind}_super_summary.csv")
        vot_out = os.path.join(out_dir, f"{field}.{sbid}_obs_{kind}_super_summary.vot")
        T.write(csv_out, format="csv", overwrite=True)
        T.write(vot_out, format="votable", overwrite=True)
        print(f"[ObsSuper] No input VOTs found. Wrote empty outputs to {out_dir}")
        return T

    rows: List[Dict[str, Any]] = []
    field_for_name = None
    sbid_for_name = None

    required = {"ra_deg", "dec_deg", "max_snr"}
    optional_strings = {
        "srcname","ra_hms","dec_dms",
        "beams_all","width_samples_all",
        "max_snr_event_beams","max_snr_event_widths",
        "max_snr_beam",
    }
    optional_numbers = {"max_snr_width", "max_snr_time_center"}

    for p in files:
        # FIX: require='super' for super-summary files
        meta = parse_candidate_filename(p, require='super')
        f_field = meta['field']; f_sbid = meta['sbid']; f_scan = meta['scan_id']; f_kind = meta['kind']
        if f_kind != kind:
            continue
        if field_for_name is None and f_field:
            field_for_name = f_field
        if sbid_for_name is None and f_sbid:
            sbid_for_name = f_sbid

        try:
            tab = Table.read(p, format="votable")
        except Exception as e:
            print(f"[ObsSuper] Skipping '{p}' (read error: {e})")
            continue

        missing = [c for c in required if c not in tab.colnames]
        if missing:
            print(f"[ObsSuper] '{p}' missing required columns {missing}; skipping.")
            continue

        for r in tab:
            d = {name: r[name] if name in tab.colnames else None
                 for name in itertools.chain(required, optional_strings, optional_numbers)}
            d["scan_id"] = f_scan
            rows.append(d)

    if len(rows) == 0:
        print("[ObsSuper] No valid rows after reading inputs; writing empty outputs.")
        empty_cols = [
            "source_id","srcname","ra_deg","dec_deg","ra_hms","dec_dms",
            "max_snr","max_snr_time_center","max_snr_beam","n_scans",
            "scan_ids","beams_all",
        ]
        if kind == "boxcar":
            empty_cols += ["max_snr_width","width_samples_all"]
        T = Table(names=empty_cols)
        field = field_for_name or "field"
        sbid = sbid_for_name or os.path.basename(os.path.abspath(obs_root))
        csv_out = os.path.join(out_dir, f"{field}.{sbid}_obs_{kind}_super_summary.csv")
        vot_out = os.path.join(out_dir, f"{field}.{sbid}_obs_{kind}_super_summary.vot")
        T.write(csv_out, format="csv", overwrite=True)
        T.write(vot_out, format="votable", overwrite=True)
        return T

    # Sky-only clustering across all scans
    _ensure_sexagesimal_on_rows(rows)
    ra = np.array([float(r["ra_deg"]) for r in rows], dtype=float)
    dec = np.array([float(r["dec_deg"]) for r in rows], dtype=float)
    comps = _cluster_indices_by_sky(ra, dec, seplimit_arcsec=sky_tol_arcsec)

    obs_rows = []
    for comp_idxs in comps:
        sub = [rows[i] for i in comp_idxs]

        # Representative = max SNR
        best = max(sub, key=lambda d: float(d.get("max_snr", -np.inf)))
        best_ra = float(best["ra_deg"]); best_dec = float(best["dec_deg"])
        best_snr = float(best.get("max_snr", np.nan))
        best_time = float(best.get("max_snr_time_center", np.nan)) if best.get("max_snr_time_center", None) is not None else np.nan
        best_scan = str(best.get("scan_id", ""))

        # Fill sexagesimal/name if missing
        ra_hms = str(best.get("ra_hms", "") or "")
        dec_dms = str(best.get("dec_dms", "") or "")
        srcname = str(best.get("srcname", "") or "")
        if not (ra_hms and dec_dms and srcname):
            ra_hms_, dec_dms_, srcname_ = _sexagesimal_from_deg(best_ra, best_dec)
            ra_hms = ra_hms or ra_hms_
            dec_dms = dec_dms or dec_dms_
            srcname = srcname or srcname_

        scan_ids = sorted({str(r.get("scan_id", "")) for r in sub if r.get("scan_id", "")})
        beams_union = set()
        for r in sub:
            beams_union |= set(_split_list_str(r.get("beams_all", "")))

        row_out = {
            "srcname": srcname,
            "ra_deg": best_ra,
            "dec_deg": best_dec,
            "ra_hms": ra_hms,
            "dec_dms": dec_dms,
            "max_snr": best_snr,
            "max_snr_time_center": best_time,
            "max_snr_beam": str(best.get("max_snr_beam", "")) if best.get("max_snr_beam", "") else "",
            "n_scans": len(scan_ids),
            "scan_ids": ",".join(scan_ids),
            "beams_all": ",".join(sorted([b for b in beams_union if b])),
            "max_snr_scan_id": best_scan,
        }

        if kind == "boxcar":
            widths_union = set()
            for r in sub:
                widths_union |= set(_split_list_int(r.get("width_samples_all", "")))
            max_w = -1
            if best.get("max_snr_width", None) not in (None, ""):
                try:
                    max_w = int(best["max_snr_width"])
                except Exception:
                    max_w = -1
            elif best.get("max_snr_event_widths", ""):
                ws = _split_list_int(best.get("max_snr_event_widths", ""))
                if len(ws) == 1:
                    max_w = ws[0]
            row_out["max_snr_width"] = int(max_w) if max_w is not None else -1
            row_out["width_samples_all"] = ",".join(str(w) for w in sorted(widths_union))

        obs_rows.append(row_out)

    # Sort, id, and write
    obs_rows.sort(key=lambda d: float(d.get("max_snr", -np.inf)), reverse=True)
    for i, d in enumerate(obs_rows):
        d["source_id"] = i

    cols = [
        "source_id","srcname","ra_deg","dec_deg","ra_hms","dec_dms",
        "max_snr","max_snr_time_center","max_snr_beam",
        "max_snr_scan_id",
        "n_scans","scan_ids","beams_all",
    ]
    if kind == "boxcar":
        cols += ["max_snr_width","width_samples_all"]
    out_tab = Table(rows=[[r.get(c) for c in cols] for r in obs_rows], names=cols)

    try:
        out_tab = annotate_observation_with_catalogs(
            out_tab,
            psrcat_csv_path=psrcat_csv_path,
            racs_vot_path=racs_vot_path,
            match_radius_arcsec=match_radius_arcsec,
            simbad_enable=simbad_enable,
        )
    except Exception as e:
        print(f"[ObsSuper] WARNING: Crossmatch step failed. Continuing without: {e}")

    field = field_for_name or "field"
    sbid = sbid_for_name or os.path.basename(os.path.abspath(obs_root))
    csv_out = os.path.join(out_dir, f"{field}.{sbid}_obs_{kind}_super_summary.csv")
    vot_out = os.path.join(out_dir, f"{field}.{sbid}_obs_{kind}_super_summary.vot")
    out_tab.write(csv_out, format="csv", overwrite=True)
    out_tab.write(vot_out, format="votable", overwrite=True)
    print(f"[ObsSuper] Wrote {len(out_tab)} sources -> {csv_out}, {vot_out}")
    return out_tab
